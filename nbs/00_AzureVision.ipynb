{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4f63d-d65f-45b9-8a89-a49dc0a2e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp AzureVision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17ba8c",
   "metadata": {},
   "source": [
    "# Azure Vision Implementaion - Dima "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18180f",
   "metadata": {},
   "source": [
    "This notebook utilizes Azure AI Document Intelligence Studio to extract text from a set of Herbarium specimens. There was a previous issue with high quality images being too large for Azure to process, this has been fixed through the resize_image function in this notebook that converts all images to 4mb or less. We have now been able to resize all images in the /projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/ folder, same folder used by the previous semester's group testing their TROCR models. \n",
    "\n",
    "Currently: the notebook takes an input image from: /projectnb/sparkgrp/ml-herbarium-grp/fall2023/LLM_Specimens, runs it through Azure Vision, analyzes all text, creates a pdf with the original image, an annotated image that has boxes around identified words and predicted words written over the original text. Below the image the entire text identified is printed along with the confidence score for each identified term. All this is saved and stored in: /projectnb/sparkgrp/ml-herbarium-grp/fall2023/AzureVision-results.\n",
    "\n",
    "-Previously there was a text recognition issue with images that have both text and the plant itself, this has been resolved.\n",
    "-Experimentation was conducted to see if a custom Azure AI model could be trained to extract Taxon, collector, date, and geography data, this proved to be inefficient and the quality was poor.\n",
    "-As a result the model will focus on extracting all the text from each image and using Open AI's ChatGPT to process the text into a Darwin JSON format. \n",
    "\n",
    "We are also looking into validating our results. We are looking into the validation dataset used by the group that created the TROCR model and we are also considering creating our own validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2d218",
   "metadata": {},
   "source": [
    "For the sake of presentation we are also looking into creating a simple user friendly demo app that will enable user to input a Herbarium sample, press a button, and see the processed result- the Taxon, Collection Date, Collector Name, and the Geography. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#!pip install azure-ai-formrecognizer --pre\n",
    "#!pip install opencv-python-headless matplotlib\n",
    "#!pip install matplotlib pillow\n",
    "#!pip install ipywidgets\n",
    "#!pip install shapely\n",
    "#!pip install openai\n",
    "#!pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773366f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from PIL import Image\n",
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import openai\n",
    "import re\n",
    "import os\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n",
    "# import gradio as gr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179830cd",
   "metadata": {},
   "source": [
    "# Resizing images to smaller size for API to accept them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808c408-6b31-4e09-ac0b-96d0945b4ff9",
   "metadata": {},
   "source": [
    "#| export\n",
    "\n",
    "def resize_image(input_path, output_path, max_size_mb, quality=85):\n",
    "\n",
    "    \"\"\"\n",
    "    Resize the image found at input_path and save it to output_path.\n",
    "    The image is resized to be under max_size_mb megabytes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    with Image.open(input_path) as img:\n",
    "        # Calculate target size to maintain aspect ratio\n",
    "        ratio = img.width / img.height\n",
    "        target_width = int((max_size_mb * 1024 * 1024 * ratio) ** 0.5)\n",
    "        target_height = int(target_width / ratio)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img = img.resize((target_width, target_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Save the resized image\n",
    "        resized_img.save(output_path, quality=quality)\n",
    "\n",
    "input_folder = 'data/goodfiles/'\n",
    "\n",
    "output_folder = 'data/resized-images/'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "    # Check if the file is an image\n",
    "\n",
    "    if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "    \n",
    "        resize_image(file_path, output_file_path, max_size_mb=4)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cebe57",
   "metadata": {},
   "source": [
    "The code below sets up a connection to Azure Cognitive Services for document analysis, and includes functions for sanitizing filenames and formatting bounding boxes.\n",
    "\n",
    "It defines a function to annotate images with extracted text and bounding boxes, and another function to parse document content using GPT-4.\n",
    "\n",
    "The main function, analyze_read, reads images, extracts text using Azure, annotates these images, and creates a PDF report that includes both the original and annotated images, along with the extracted text.\n",
    "\n",
    "The results are saved in /projectnb/sparkgrp/ml-herbarium-grp/fall2023/AzureVision-results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef5e22",
   "metadata": {},
   "source": [
    "# Extracting Relevant Info w/ GPT-4 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5cf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"occurrenceID\": \"00592128\",\n",
      "  \"scientificName\": \"Atriplex patula L. var. hastata Gray\",\n",
      "  \"recordedBy\": \"Kate Furbish\",\n",
      "  \"country\": \"USA\",\n",
      "  \"stateProvince\": \"Maine\",\n",
      "  \"county\": \"Washington\",\n",
      "  \"locality\": \"North Lubec\",\n",
      "  \"eventDate\": \"1902-09-13\",\n",
      "  \"institutionCode\": \"Harvard University Herbaria\",\n",
      "  \"catalogNumber\": \"00592128\",\n",
      "  \"basisOfRecord\": \"PreservedSpecimen\",\n",
      "  \"rightsHolder\": \"President and Fellows of Harvard College\"\n",
      "}\n",
      "```\n",
      "Below is the JSON representation of the relevant Darwin Core fields extracted from the provided text. The Darwin Core standard has many possible fields, but I've put together the ones that can be inferred from the text you've supplied:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"type\": \"SYNTYPE\",\n",
      "  \"scientificName\": \"Glaux maritima var. obtusifolia Fernald\",\n",
      "  \"acceptedNameUsage\": \"Glaux maritima L.\",\n",
      "  \"verbatimEventDate\": \"July 24, 1902\",\n",
      "  \"recordNumber\": \"00936578, 00936579\",\n",
      "  \"locality\": \"Brackish marsh, St. Andrews, BATHURST, GLOUCESTER COUNTY\",\n",
      "  \"stateProvince\": \"New Brunswick\",\n",
      "  \"country\": \"Canada\",\n",
      "  \"identifiedBy\": \"Walter T. Kittredge\",\n",
      "  \"dateIdentified\": \"2019\",\n",
      "  \"recordedBy\": \"E. F. Williams, M. L. Fernald\",\n",
      "  \"institutionCode\": \"Harvard University Herbaria\",\n",
      "  \"catalogNumber\": \"Not provided (use recordNumber instead)\",\n",
      "  \"publicationCitation\": \"Fernald, Rhodora 4: 215. 1902\",\n",
      "  \"basisOfRecord\": \"PreservedSpecimen\",\n",
      "  \"occurrenceID\": \"Not provided\",\n",
      "  \"higherGeography\": \"North America\",\n",
      "  \"coordinateUncertaintyInMeters\": \"Not provided\"\n",
      "}\n",
      "```\n",
      "\n",
      "Please note that some fields usually found in Darwin Core records such as `occurrenceID` and an exact `catalogNumber` cannot be reliably derived from the text provided. Additionally, the common name and higher taxonomy are not specified in the text. Where confidence levels were low or information could not be precisely determined from the text (e.g., exact coordinates, altitude), the \"Not provided\" value is used to indicate missing data.\n",
      "```json\n",
      "{\n",
      "  \"occurrenceID\": \"3040830\",\n",
      "  \"institutionCode\": \"NATIONAL HERBARIUM UNITED STATES\",\n",
      "  \"collectionCode\": \"PLANTS OF ILLINOIS\",\n",
      "  \"catalogNumber\": \"04385325\",\n",
      "  \"scientificName\": \"Sporobolus clandestinus (Spreng) Hitche\",\n",
      "  \"country\": \"US\",\n",
      "  \"stateProvince\": \"Illinois\",\n",
      "  \"county\": \"Henderson\",\n",
      "  \"locality\": \"Sandy bluff of Mississippi river, north of Oquawka\",\n",
      "  \"eventDate\": \"1941-08-27\",\n",
      "  \"recordedBy\": \"D. E. AND M. S. EYLES\",\n",
      "  \"recordNumber\": \"389\",\n",
      "  \"decimalLatitude\": \"\",\n",
      "  \"decimalLongitude\": \"\",\n",
      "  \"coordinateUncertaintyInMeters\": \"\",\n",
      "  \"identifiedBy\": \"\",\n",
      "  \"dateIdentified\": \"\",\n",
      "  \"basisOfRecord\": \"PreservedSpecimen\",\n",
      "  \"individualCount\": \"\",\n",
      "  \"preparations\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Note**: \n",
      "\n",
      "- The `decimalLatitude` and `decimalLongitude` fields did not contain valid entries in the provided text that could be extracted with confidence. Hence, they were left empty.\n",
      "- Similarly, no data was given for the `coordinateUncertaintyInMeters`, `identifiedBy`, `dateIdentified`, `individualCount`, and `preparations`. These fields must be populated with relevant data if available, or left empty/blanks as shown.\n",
      "- The format of `eventDate` was standardized to an ISO 8601 format (YYYY-MM-DD).\n",
      "- `basisOfRecord` was assumed to be \"PreservedSpecimen\" as the standard for physical specimens. If this is not the case, it should be adjusted accordingly based on the exact nature of the record.\n",
      "\n",
      "Please make sure to validate the specifics (e.g., precise latitude/longitude, identifier's credentials, date of identification, etc.) and fill in any missing or uncertain parts from the original data if necessary.\n",
      "```json\n",
      "{\n",
      "  \"occurrenceID\": \"03587160\",\n",
      "  \"institutionCode\": \"Smithsonian Institution\",\n",
      "  \"collectionCode\": \"United States National Herbarium\",\n",
      "  \"basisOfRecord\": \"PreservedSpecimen\",\n",
      "  \"scientificName\": \"Nasturtium palustre (L.) DC.\",\n",
      "  \"acceptedNameUsage\": \"Rorippa islandica (Oeder ex Murray) Borbás\",\n",
      "  \"family\": \"Brassicaceae\",\n",
      "  \"eventDate\": \"1965-05-22\",\n",
      "  \"year\": \"1965\",\n",
      "  \"month\": \"5\",\n",
      "  \"day\": \"22\",\n",
      "  \"country\": \"United States of America\",\n",
      "  \"stateProvince\": \"Nebraska\",\n",
      "  \"county\": \"Cedar Co.\",\n",
      "  \"recordedBy\": \"Fred Clements\",\n",
      "  \"identifiedBy\": \"Ronald L. Stuckey\",\n",
      "  \"catalogNumber\": \"2618\",\n",
      "  \"otherCatalogNumbers\": \"219017\",\n",
      "  \"decimalLatitude\": \"\",\n",
      "  \"decimalLongitude\": \"\",\n",
      "  \"coordinateUncertaintyInMeters\": \"\",\n",
      "  \"verbatimCoordinates\": \"\",\n",
      "  \"geodeticDatum\": \"\",\n",
      "  \"verbatimSRS\": \"\",\n",
      "  \"informationWithheld\": \"\",\n",
      "  \"dataGeneralizations\": \"\",\n",
      "  \"dynamicProperties\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "Note: Coordinates and some other specific details like uncertainty and datum were not included in the provided text, so corresponding fields are left blank. The accepted name usage is assumed as per the identifier's determination. The collector's full name appears to be 'Fred Clements' based on the different notations. The confidence values provided do not affect the Darwin Core formatted data unless it specifically impacts the interpretation of relevant fields.\n",
      "```json\n",
      "{\n",
      "  \"occurrenceID\": \"1080232\",\n",
      "  \"institutionCode\": \"University of Minnesota\",\n",
      "  \"collectionCode\": \"Herbarium\",\n",
      "  \"recordedBy\": \"Sharon S. & Arthur O. Tucker\",\n",
      "  \"recordNumber\": \"181466\",\n",
      "  \"year\": \"1993\",\n",
      "  \"scientificName\": \"Blephilia hirsuta (Pursh) Benth.\",\n",
      "  \"locality\": \"Houston Co.\",\n",
      "  \"eventDate\": \"1899-07-11\",\n",
      "  \"country\": \"USA\",\n",
      "  \"stateProvince\": \"Minnesota\",\n",
      "  \"county\": \"Houston\",\n",
      "  \"catalogNumber\": \"181466\",\n",
      "  \"identifiedBy\": \"H.S. Lyon\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"your key here\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    return \", \".join([\"[{}, {}]\".format(p.x, p.y) for p in bounding_box])\n",
    "\n",
    "\n",
    "def draw_boxes(image_path, words):\n",
    "    original_image = Image.open(image_path)\n",
    "    annotated_image = original_image.copy()\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    for word in words:\n",
    "        polygon = word['polygon']\n",
    "        if polygon:\n",
    "            bbox = [(point.x, point.y) for point in polygon]\n",
    "            try:\n",
    "                # Replace special characters that cannot be encoded in 'latin-1'\n",
    "                text_content = word['content'].encode('ascii', 'ignore').decode('ascii')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {word['content']}: {e}\")\n",
    "                text_content = \"Error\"\n",
    "            draw.polygon(bbox, outline=\"red\")\n",
    "            draw.text((bbox[0][0], bbox[0][1]), text_content, fill=\"green\")\n",
    "\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "page_width, page_height = letter \n",
    "\n",
    "# Function to calculate scale to fit the image within page dimensions\n",
    "def calculate_scale(image, max_width, max_height):\n",
    "    scale_w = max_width / image.width\n",
    "    scale_h = max_height / image.height\n",
    "    return min(scale_w, scale_h)\n",
    "\n",
    "\n",
    "def extract_info(text):\n",
    "    # Set your OpenAI API key\n",
    "    openai.api_key = 'your key here'\n",
    "\n",
    "    # Prepare the prompt for the API\n",
    "    prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen:\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        # Send the request to the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "        if response.choices:\n",
    "            return response.choices[0].message['content']\n",
    "        else:\n",
    "            return \"No response from the API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "\n",
    "def analyze_read(image_path, output_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "       # Collect words, their polygon data, and confidence\n",
    "        words = []\n",
    "        confidence_text = \"\"\n",
    "        for page in result.pages:\n",
    "            for word in page.words:\n",
    "                words.append({\n",
    "                    'content': word.content,\n",
    "                    'polygon': word.polygon\n",
    "                })\n",
    "                confidence_text += \"'{}' confidence {}\\n\".format(word.content, word.confidence)\n",
    "\n",
    "        document_content = result.content + \"\\n\\nConfidence Metrics:\\n\" + confidence_text\n",
    "        extracted_info = extract_info(document_content)\n",
    "        print(extracted_info)\n",
    "        original_image = Image.open(image_path)\n",
    "        annotated_img = draw_boxes(image_path, words)\n",
    "        \n",
    "\n",
    "        # Set up PDF\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.pdf')))\n",
    "        c = canvas.Canvas(output_filename, pagesize=letter)\n",
    "        width, height = letter  # usually 612 x 792\n",
    "\n",
    "        # Draw original image\n",
    "        if original_image.height <= height:\n",
    "            c.drawImage(image_path, 0, height - original_image.height, width=original_image.width, height=original_image.height, mask='auto')\n",
    "            y_position = height - original_image.height\n",
    "        else:\n",
    "            # Handle large images or add scaling logic here\n",
    "            pass\n",
    "\n",
    "        \n",
    "        # Draw original image\n",
    "        scale = calculate_scale(original_image, page_width, page_height)\n",
    "        img_width, img_height = original_image.width * scale, original_image.height * scale\n",
    "        c.drawImage(image_path, 0, page_height - img_height, width=img_width, height=img_height, mask='auto')\n",
    "        y_position = page_height - img_height\n",
    "\n",
    "        # Draw annotated image\n",
    "        annotated_image_path = '/tmp/annotated_image.png'\n",
    "        annotated_img.save(annotated_image_path)\n",
    "        scale = calculate_scale(annotated_img, page_width, page_height)\n",
    "        annotated_img_width, annotated_img_height = annotated_img.width * scale, annotated_img.height * scale\n",
    "        if y_position - annotated_img_height >= 0:\n",
    "            c.drawImage(annotated_image_path, 0, y_position - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        else:\n",
    "            c.showPage()\n",
    "            c.drawImage(annotated_image_path, 0, page_height - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        \n",
    "\n",
    "        # Add text\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(10, y_position - 15)\n",
    "        textobject.setFont(\"Times-Roman\", 12)\n",
    "\n",
    "        for line in document_content.split('\\n'):\n",
    "            if textobject.getY() - 15 < 0:  # Check if new page is needed for more text\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                textobject = c.beginText()\n",
    "                textobject.setTextOrigin(10, height - 15)\n",
    "                textobject.setFont(\"Times-Roman\", 12)\n",
    "            textobject.textLine(line)\n",
    "        \n",
    "        c.drawText(textobject)\n",
    "        c.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    input_folder = 'data/resized-images/'\n",
    "    output_folder = 'data/temp/'\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 5:  # Stop after processing x images\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb317d2a",
   "metadata": {},
   "source": [
    "# Saving as a .txt instead of a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6488535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"your key here\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def extract_info(text):\n",
    "    # Set your OpenAI API key\n",
    "    openai.api_key = 'your key here'\n",
    "\n",
    "    # Prepare the prompt for the API\n",
    "    prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen. Note: make sure that each output has a 'country' field. If you do not find an explicit country, make your best guess at the country using the context of the other text.\\n{text}\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        # Send the request to the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "        return response.choices[0].message['content'] if response.choices else \"No response from the API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def analyze_read(image_path, output_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect the content from the document\n",
    "        document_content = result.content\n",
    "        extracted_info = extract_info(document_content)\n",
    "\n",
    "        # Save the extracted information to a text file\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.txt')))\n",
    "        with open(output_filename, 'w') as text_file:\n",
    "            text_file.write(extracted_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    input_folder = 'data/resized-images/'\n",
    "    output_folder = 'data/AzureVisionResults/'\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 224:  # Stop after processing x images\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f69d7b",
   "metadata": {},
   "source": [
    "# Making A UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34619cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8.0\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#!pip install --upgrade gradio\n",
    "print(gr.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbd17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://2f755593139e8ed224.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2f755593139e8ed224.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"your key here\"\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def extract_info(text):\n",
    "    # Set your OpenAI API key\n",
    "    openai.api_key = 'your key here'\n",
    "\n",
    "    # Prepare the prompt for the API\n",
    "    prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen. Note: make sure that each output has a 'country' field. If you do not find an explicit country, make your best guess at the country using the context of the other text.\\n{text}\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        # Send the request to the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "        return response.choices[0].message['content'] if response.choices else \"No response from the API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def analyze_read(image_stream):\n",
    "    try:\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect the content from the document\n",
    "        document_content = result.content\n",
    "        extracted_info = extract_info(document_content)\n",
    "\n",
    "        return extracted_info\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def model_function(image):\n",
    "    # Convert the NumPy array to a PIL Image object\n",
    "    image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "\n",
    "    # Convert the uploaded image to a byte stream\n",
    "    image_bytes = io.BytesIO()\n",
    "    image.save(image_bytes, format='JPEG')  # Using 'JPEG' as the format\n",
    "    image_bytes = image_bytes.getvalue()\n",
    "\n",
    "    output_text = analyze_read(image_bytes)\n",
    "    return output_text\n",
    "\n",
    "# iface = gr.Interface(fn=model_function, inputs=\"image\", outputs=\"text\")\n",
    "# iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791a84b",
   "metadata": {},
   "source": [
    "# Experimenting With Cropping Images\n",
    "\n",
    "So far results have been better for images that have not been cropped (cropping function needs improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"AZURE KEY HERE\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    return \", \".join([\"[{}, {}]\".format(p.x, p.y) for p in bounding_box])\n",
    "\n",
    "def draw_boxes(image_path, words):\n",
    "    original_image = Image.open(image_path)\n",
    "    annotated_image = original_image.copy()\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    for word in words:\n",
    "        polygon = word['polygon']\n",
    "        if polygon:\n",
    "            bbox = [(point.x, point.y) for point in polygon]\n",
    "            try:\n",
    "                # Replace special characters that cannot be encoded in 'latin-1'\n",
    "                text_content = word['content'].encode('ascii', 'ignore').decode('ascii')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {word['content']}: {e}\")\n",
    "                text_content = \"Error\"\n",
    "            draw.polygon(bbox, outline=\"red\")\n",
    "            draw.text((bbox[0][0], bbox[0][1]), text_content, fill=\"green\")\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "page_width, page_height = letter \n",
    "\n",
    "# Function to calculate scale to fit the image within page dimensions\n",
    "def calculate_scale(image, max_width, max_height):\n",
    "    scale_w = max_width / image.width\n",
    "    scale_h = max_height / image.height\n",
    "    return min(scale_w, scale_h)\n",
    "\n",
    "\n",
    "def get_text_density_map(pages):\n",
    "    density_map = {}\n",
    "    for page in pages:\n",
    "        for line in page.lines:\n",
    "            points = line.polygon\n",
    "            if points:\n",
    "                x_center = sum(point.x for point in points) / len(points)\n",
    "                y_center = sum(point.y for point in points) / len(points)\n",
    "                density_map[(x_center, y_center)] = density_map.get((x_center, y_center), 0) + 1\n",
    "    return density_map\n",
    "\n",
    "def find_highest_density_area(density_map):\n",
    "    # This function will find the center of the area with the highest text density\n",
    "    # For simplicity, this example just returns the center with the highest count\n",
    "    # In a real scenario, you might want to consider a more sophisticated method\n",
    "    # that takes into account the size and proximity of the high-density areas\n",
    "    return max(density_map, key=density_map.get)\n",
    "\n",
    "def crop_image_to_text(image_path, density_center, crop_size):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Calculate the coordinates for the crop\n",
    "        left = max(density_center[0] - crop_size[0] // 2, 0)\n",
    "        upper = max(density_center[1] - crop_size[1] // 2, 0)\n",
    "        right = min(density_center[0] + crop_size[0] // 2, img.width)\n",
    "        lower = min(density_center[1] + crop_size[1] // 2, img.height)\n",
    "\n",
    "        # Debug output\n",
    "        print(f\"Cropping coordinates: left={left}, upper={upper}, right={right}, lower={lower}\")\n",
    "\n",
    "        # Perform the crop\n",
    "        cropped_img = img.crop((left, upper, right, lower))\n",
    "        return cropped_img\n",
    "    \n",
    "    \n",
    "def get_text_bounding_boxes(pages):\n",
    "    bounding_boxes = []\n",
    "    for page in pages:\n",
    "        for line in page.lines:\n",
    "            if line.polygon:\n",
    "                box = [(point.x, point.y) for point in line.polygon]\n",
    "                bounding_boxes.append(box)\n",
    "    return bounding_boxes\n",
    "\n",
    "def combine_text_regions(image_path, bounding_boxes):\n",
    "    original_image = Image.open(image_path)\n",
    "\n",
    "    # Calculate the combined bounding box\n",
    "    min_x = min(min(box, key=lambda x: x[0])[0] for box in bounding_boxes)\n",
    "    min_y = min(min(box, key=lambda x: x[1])[1] for box in bounding_boxes)\n",
    "    max_x = max(max(box, key=lambda x: x[0])[0] for box in bounding_boxes)\n",
    "    max_y = max(max(box, key=lambda x: x[1])[1] for box in bounding_boxes)\n",
    "\n",
    "    # Create a new blank image with integer dimensions\n",
    "    combined_image = Image.new('RGB', (int(max_x - min_x), int(max_y - min_y)), (255, 255, 255))\n",
    "    \n",
    "    for box in bounding_boxes:\n",
    "        cropped_region = original_image.crop((min(box, key=lambda x: x[0])[0], \n",
    "                                              min(box, key=lambda x: x[1])[1], \n",
    "                                              max(box, key=lambda x: x[0])[0], \n",
    "                                              max(box, key=lambda x: x[1])[1]))\n",
    "        # Paste the cropped region at integer coordinates\n",
    "        combined_image.paste(cropped_region, (int(box[0][0] - min_x), int(box[0][1] - min_y)))\n",
    "\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "def parse_document_content(content):\n",
    "    openai.api_key = 'your-api-key'\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-4\",\n",
    "            prompt=f\"Extract specific information from the following text: {content}\\n\\nSpecies Name: \",\n",
    "            max_tokens=100\n",
    "            # Add additional parameters as needed\n",
    "        )\n",
    "        parsed_data = response.choices[0].text.strip()\n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "####################################################\n",
    "\n",
    "def analyze_text_density_and_crop(image_path):\n",
    "    document_analysis_client = DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "    \n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_stream = f.read()\n",
    "\n",
    "    poller = document_analysis_client.begin_analyze_document(\"prebuilt-read\", image_stream)\n",
    "    result = poller.result()\n",
    "\n",
    "    # Get bounding boxes of text regions\n",
    "    bounding_boxes = get_text_bounding_boxes(result.pages)\n",
    "\n",
    "    # Combine the text regions into one image\n",
    "    combined_image = combine_text_regions(image_path, bounding_boxes)\n",
    "\n",
    "    # Save the combined image temporarily and return its path\n",
    "    combined_image_path = '/tmp/combined_image.png'\n",
    "    combined_image.save(combined_image_path)\n",
    "    return combined_image_path\n",
    "\n",
    "\n",
    "def analyze_read(image_path, output_path, show_first_output=False):\n",
    "    combined_image_path = analyze_text_density_and_crop(image_path)\n",
    "\n",
    "    try:\n",
    "        # Process the combined image with Azure Form Recognizer\n",
    "        with open(combined_image_path, \"rb\") as f:\n",
    "            combined_image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", combined_image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect words, their polygon data, and confidence\n",
    "        words = []\n",
    "        for page in result.pages:\n",
    "            for word in page.words:\n",
    "                words.append({\n",
    "                    'content': word.content,\n",
    "                    'polygon': word.polygon\n",
    "                })\n",
    "\n",
    "        # Prepare annotated image\n",
    "        annotated_img = draw_boxes(combined_image_path, words)\n",
    "\n",
    "        # Set up PDF\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.pdf')))\n",
    "        c = canvas.Canvas(output_filename, pagesize=letter)\n",
    "\n",
    "        # Draw original image\n",
    "        original_image = Image.open(image_path)\n",
    "        scale = calculate_scale(original_image, page_width, page_height)\n",
    "        img_width, img_height = original_image.width * scale, original_image.height * scale\n",
    "        c.drawImage(image_path, 0, page_height - img_height, width=img_width, height=img_height, mask='auto')\n",
    "        y_position = page_height - img_height\n",
    "\n",
    "        # Draw annotated combined image\n",
    "        annotated_image_path = '/tmp/annotated_image.png'\n",
    "        annotated_img.save(annotated_image_path)\n",
    "        scale = calculate_scale(annotated_img, page_width, page_height)\n",
    "        annotated_img_width, annotated_img_height = annotated_img.width * scale, annotated_img.height * scale\n",
    "        if y_position - annotated_img_height >= 0:\n",
    "            c.drawImage(annotated_image_path, 0, y_position - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        else:\n",
    "            c.showPage()  # Start a new page if not enough space\n",
    "            c.drawImage(annotated_image_path, 0, page_height - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "\n",
    "        # Add text\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(10, y_position - 15)\n",
    "        textobject.setFont(\"Times-Roman\", 12)\n",
    "\n",
    "        document_content = '\\n'.join([word['content'] for word in words])\n",
    "        for line in document_content.split('\\n'):\n",
    "            if textobject.getY() - 15 < 0:  # Check if new page is needed for more text\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                textobject = c.beginText()\n",
    "                textobject.setTextOrigin(10, page_height - 15)\n",
    "                textobject.setFont(\"Times-Roman\", 12)\n",
    "            textobject.textLine(line)\n",
    "\n",
    "        c.drawText(textobject)\n",
    "        c.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {combined_image_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    input_folder = 'data/resized-images/'\n",
    "    output_folder = 'data/AzureVisioncrop/'\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder, show_first_output=not first_output_shown)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 1:  # Stop after processing 5 images\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95025fa1",
   "metadata": {},
   "source": [
    "# Running Model On Cyrillic & Chinese Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75632e6f-35cd-4bc6-889b-dc2628d1c7ad",
   "metadata": {},
   "source": [
    "Azure Cognitive Services endpoint and key\n",
    "\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "\n",
    "key = \"your key here\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def extract_info(text):\n",
    "\n",
    "    # Set your OpenAI API key\n",
    "\n",
    "    openai.api_key = 'your key here'\n",
    "\n",
    "    # Prepare the prompt for the API\n",
    "\n",
    "    #prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen. NOTE: Parts or the majority of the textmay be in Cyrillic. Take this into consideration. Additionally, there should be a 'country' field, if you cannot determinethe country directly, to your best ability, infer what the country is:\\n{text}\"\n",
    "    prompt = f\"From the provided text, return only the relevant information in a JSON format according to the Darwin Core standard for biodiversity specimen. NOTE: Parts or the majority of the textmay be in Chinese, Take this into consideration. Additionally, there should be a 'country' field, if you cannot determinethe country directly, to your best ability, infer what the country is. Lastly, the more info the better, output chinese character as appropriate:\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        # Send the request to the API\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "\n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response\n",
    "\n",
    "        return response.choices[0].message['content'] if response.choices else \"No response from the API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def analyze_read(image_path, output_path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect the content from the document\n",
    "\n",
    "        document_content = result.content\n",
    "        extracted_info = extract_info(document_content)\n",
    "        \n",
    "        #print(extracted_info)\n",
    "\n",
    "\n",
    "        # Save the extracted information to a text file\n",
    "\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.txt')))\n",
    "        with open(output_filename, 'w') as text_file:\n",
    "            text_file.write(extracted_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    input_folder = '/data/CyrillicImages/'\n",
    "\n",
    "    output_folder = '/data/CyrillicResults/'\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        \n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 35:  # Stop after processing x images\n",
    "                break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
